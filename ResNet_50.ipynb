{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/himanshu11081999/ResNet-50/blob/main/ResNet_50.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython import get_ipython\n",
        "from IPython.display import display\n",
        "# %%\n",
        "# Install necessary libraries if not already present\n",
        "!pip install torch torchvision scikit-learn kagglehub\n",
        "\n",
        "# %%\n",
        "!pip install medmnist\n",
        "# %%\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, models, transforms\n",
        "from sklearn.metrics import f1_score, roc_auc_score\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "import os\n",
        "import shutil # Import shutil for removing directories\n",
        "import tarfile # Import tarfile for extracting .tar.gz files\n",
        "import kagglehub # Import kagglehub for dataset download\n",
        "import medmnist # Import medmnist\n",
        "from medmnist.dataset import PneumoniaMNIST # Import the specific dataset class\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6tiTyJk1FoNq",
        "outputId": "8489d466-7c09-42a6-f8d6-23aa84c6bcee"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: kagglehub in /usr/local/lib/python3.11/dist-packages (0.3.12)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.2.1)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.0)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from kagglehub) (24.2)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from kagglehub) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from kagglehub) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from kagglehub) (4.67.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->kagglehub) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->kagglehub) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->kagglehub) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->kagglehub) (2025.4.26)\n",
            "Requirement already satisfied: medmnist in /usr/local/lib/python3.11/dist-packages (3.0.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from medmnist) (2.0.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from medmnist) (2.2.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from medmnist) (1.6.1)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.11/dist-packages (from medmnist) (0.25.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from medmnist) (4.67.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from medmnist) (11.2.1)\n",
            "Requirement already satisfied: fire in /usr/local/lib/python3.11/dist-packages (from medmnist) (0.7.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from medmnist) (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (from medmnist) (0.21.0+cu124)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.11/dist-packages (from fire->medmnist) (3.1.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->medmnist) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->medmnist) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->medmnist) (2025.2)\n",
            "Requirement already satisfied: scipy>=1.11.4 in /usr/local/lib/python3.11/dist-packages (from scikit-image->medmnist) (1.15.3)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.11/dist-packages (from scikit-image->medmnist) (3.4.2)\n",
            "Requirement already satisfied: imageio!=2.35.0,>=2.33 in /usr/local/lib/python3.11/dist-packages (from scikit-image->medmnist) (2.37.0)\n",
            "Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.11/dist-packages (from scikit-image->medmnist) (2025.5.21)\n",
            "Requirement already satisfied: packaging>=21 in /usr/local/lib/python3.11/dist-packages (from scikit-image->medmnist) (24.2)\n",
            "Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.11/dist-packages (from scikit-image->medmnist) (0.4)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->medmnist) (1.5.0)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->medmnist) (3.6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->medmnist) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch->medmnist) (4.13.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->medmnist) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->medmnist) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->medmnist) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->medmnist) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->medmnist) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch->medmnist) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch->medmnist) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch->medmnist) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch->medmnist) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch->medmnist) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch->medmnist) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->medmnist) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->medmnist) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->medmnist) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->medmnist) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch->medmnist) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->medmnist) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->medmnist) (1.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->medmnist) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->medmnist) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Dataset: PneumoniaMNIST\n",
        "# Using kagglehub to download the dataset\n",
        "\n",
        "# Define the target data directory where you want the data to reside\n",
        "data_dir = 'data/pneumoniamnist'\n",
        "dataset_file_name = 'pneumoniamnist.npz' # Expecting .npz file now\n",
        "downloaded_npz_path = os.path.join(data_dir, dataset_file_name) # Define the path for the .npz file\n",
        "\n",
        "# Check if the data directory already exists and contains data\n",
        "# Check for the existence of the .npz file within the target data_dir\n",
        "# Note: medmnist expects the .npz file in the root directory *before* processing.\n",
        "# It then extracts/loads from this .npz into memory or potentially other files\n",
        "# depending on the dataset.\n",
        "if not os.path.exists(downloaded_npz_path):\n",
        "    print(f\"Dataset not found in {data_dir}. Downloading using kagglehub...\")\n",
        "\n",
        "    try:\n",
        "        # Download the dataset using kagglehub.\n",
        "        # This downloads the dataset files to a cache location.\n",
        "        # The path returned is the root directory of the downloaded files in the cache.\n",
        "        kaggle_download_root = kagglehub.dataset_download(\"rijulshr/pneumoniamnist\")\n",
        "        print(f\"Kaggle dataset downloaded to cache: {kaggle_download_root}\")\n",
        "\n",
        "        # Locate the expected .npz file within the downloaded directory.\n",
        "        downloaded_source_npz_path = os.path.join(kaggle_download_root, dataset_file_name)\n",
        "\n",
        "\n",
        "        # Check if the expected .npz file exists in the downloaded path\n",
        "        if not os.path.exists(downloaded_source_npz_path):\n",
        "            print(f\"Error: Expected file {dataset_file_name} not found in the Kaggle download path: {kaggle_download_root}\")\n",
        "            print(\"Please check the contents of the downloaded dataset on Kaggle or inspect the downloaded path.\")\n",
        "            # Optionally, list files in the downloaded directory for inspection\n",
        "            print(\"Files found in downloaded directory:\")\n",
        "            for root, dirs, files in os.walk(kaggle_download_root):\n",
        "                level = root.replace(kaggle_download_root, '').count(os.sep)\n",
        "                indent = ' ' * 4 * (level)\n",
        "                print(f'{indent}{os.path.basename(root)}/')\n",
        "                subindent = ' ' * 4 * (level + 1)\n",
        "                for f in files:\n",
        "                    print(f'{subindent}{f}')\n",
        "            raise FileNotFoundError(f\"{dataset_file_name} not found in {kaggle_download_root}\")\n",
        "\n",
        "        print(f\"Found dataset npz file at: {downloaded_source_npz_path}\")\n",
        "\n",
        "        # Create the target data directory if it doesn't exist\n",
        "        os.makedirs(data_dir, exist_ok=True)\n",
        "\n",
        "        # **Changed:** Use shutil.copy2 instead of shutil.move\n",
        "        # shutil.move attempts to delete the source file, which is not allowed in read-only /kaggle/input\n",
        "        # shutil.copy2 copies the file metadata as well, which is generally preferred over shutil.copy\n",
        "        print(f\"Copying {downloaded_source_npz_path} to {downloaded_npz_path}\")\n",
        "        shutil.copy2(downloaded_source_npz_path, downloaded_npz_path)\n",
        "        print(\"Copy complete.\")\n",
        "\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred during kagglehub download or processing: {e}\")\n",
        "        raise # Re-raise the exception\n",
        "\n",
        "else:\n",
        "    print(f\"Dataset already found in {downloaded_npz_path}. Skipping download.\")\n",
        "\n",
        "\n",
        "# Transforms\n",
        "# MedMNIST datasets are typically grayscale (1 channel). Normalize accordingly.\n",
        "data_transforms = {\n",
        "    'train': transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(), # Converts PIL Image to Tensor (H x W x C) to (C x H x W) and scales pixels to [0, 1]\n",
        "        transforms.Lambda(lambda x: x.repeat(3, 1, 1) if x.size(0) == 1 else x), # Convert grayscale to 3 channels if needed by the model\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) # ImageNet normalization\n",
        "    ]),\n",
        "    'val': transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Lambda(lambda x: x.repeat(3, 1, 1) if x.size(0) == 1 else x), # Convert grayscale to 3 channels if needed by the model\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) # ImageNet normalization\n",
        "    ]),\n",
        "    'test': transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Lambda(lambda x: x.repeat(3, 1, 1) if x.size(0) == 1 else x), # Convert grayscale to 3 channels if needed by the model\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) # ImageNet normalization\n",
        "    ])\n",
        "}\n",
        "\n",
        "# Load the datasets using medmnist\n",
        "# PneumoniaMNIST is dataset index 5 in medmnist v2\n",
        "# DataClass = medmnist.INFO['pneumoniamnist']['python_class'] # This returns the class NAME as a string\n",
        "DataClass = PneumoniaMNIST # Assign the imported class to DataClass\n",
        "\n",
        "info = medmnist.INFO['pneumoniamnist']\n",
        "num_classes = len(info['label']) # Get the number of classes from the medmnist info\n",
        "\n",
        "# **FIX:** Create a dummy instance to trigger medmnist's internal loading/processing\n",
        "# from the .npz file we've placed in data_dir.\n",
        "# This only needs to be done once. Subsequent dataset instances will find the processed data.\n",
        "# We pass a basic transform as the actual transforms will be applied later.\n",
        "# Set download=True to trigger the loading/processing logic if the processed data isn't found.\n",
        "print(\"Checking and processing dataset files with medmnist (if necessary)...\")\n",
        "# Use a simple transform for this initial load if transforms are required by the constructor\n",
        "# otherwise, None might suffice depending on the medmnist version/dataset\n",
        "initial_transform = transforms.Compose([transforms.ToTensor()]) # Use a minimal transform\n",
        "_ = DataClass(split='train', transform=initial_transform, download=True, root=data_dir)\n",
        "print(\"Medmnist processing check complete.\")\n",
        "\n",
        "\n",
        "# Now load the actual datasets for training, validation, and testing\n",
        "# Set download=False as the data should now be processed and available in data_dir\n",
        "train_dataset = DataClass(split='train', transform=data_transforms['train'], download=False, root=data_dir)\n",
        "val_dataset = DataClass(split='val', transform=data_transforms['val'], download=False, root=data_dir)\n",
        "test_dataset = DataClass(split='test', transform=data_transforms['test'], download=False, root=data_dir)\n",
        "\n",
        "\n",
        "# Print dataset sizes and class names for verification\n",
        "print(f\"Train dataset size: {len(train_dataset)}\")\n",
        "print(f\"Validation dataset size: {len(val_dataset)}\")\n",
        "print(f\"Test dataset size: {len(test_dataset)}\")\n",
        "# MedMNIST datasets provide label information in the INFO dictionary\n",
        "print(f\"Classes: {list(info['label'].values())}\")\n",
        "print(f\"Class to index mapping: {info['label']}\")\n",
        "\n",
        "\n",
        "# Data Loaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False) # Add a loader for the test set\n",
        "\n",
        "# Load pretrained ResNet50\n",
        "# Using weights=ResNet50_Weights.DEFAULT is the modern way to load default weights\n",
        "# You need to import ResNet50_Weights\n",
        "from torchvision.models import ResNet50_Weights\n",
        "# ResNet50 expects 3 input channels, we've handled this in the transforms\n",
        "model = models.resnet50(weights=ResNet50_Weights.DEFAULT)\n",
        "\n",
        "# Freeze all layers except the final fully connected layer\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Replace the final fully connected layer\n",
        "# The number of output features should match the number of classes in your dataset.\n",
        "# num_classes is already defined from medmnist info\n",
        "model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
        "\n",
        "# Move model to GPU if available\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Fine-tuning\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "# Only optimize the parameters of the newly added final layer\n",
        "optimizer = optim.Adam(model.fc.parameters(), lr=1e-4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YSGYXKffHYpt",
        "outputId": "e31ca7e9-b9ad-4564-cb39-376ec3cc6f37"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset already found in data/pneumoniamnist/pneumoniamnist.npz. Skipping download.\n",
            "Checking and processing dataset files with medmnist (if necessary)...\n",
            "Medmnist processing check complete.\n",
            "Train dataset size: 4708\n",
            "Validation dataset size: 524\n",
            "Test dataset size: 624\n",
            "Classes: ['normal', 'pneumonia']\n",
            "Class to index mapping: {'0': 'normal', '1': 'pneumonia'}\n",
            "Using device: cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Training Loop\n",
        "num_epochs = 1 # Define the number of epochs\n",
        "for epoch in range(num_epochs):\n",
        "    # Ensure the cell defining 'model' has been run before this cell\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct_train = 0\n",
        "    total_train = 0\n",
        "\n",
        "    for inputs, labels in train_loader:\n",
        "        # Move data to device (GPU or CPU)\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        # Squeeze the labels tensor to remove the extra dimension if it exists\n",
        "        # CrossEntropyLoss expects target shape (batch_size,)\n",
        "        if labels.ndim > 1 and labels.size(-1) == 1:\n",
        "            labels = labels.squeeze(-1)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "        # Calculate training accuracy\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total_train += labels.size(0)\n",
        "        correct_train += (predicted == labels).sum().item()\n",
        "\n",
        "\n",
        "    epoch_loss = running_loss / len(train_dataset)\n",
        "    epoch_acc = correct_train / total_train\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Training Loss: {epoch_loss:.4f}, Training Accuracy: {epoch_acc:.4f}\")"
      ],
      "metadata": {
        "id": "Ks7-9yFGEhYN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "07b313da-18ea-4a11-d299-7eda2c6b7407"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/1, Training Loss: 0.5152, Training Accuracy: 0.7477\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Collect probabilities for the positive class\n",
        "# For PneumoniaMNIST, typically label 1 corresponds to 'Pneumonia'\n",
        "if num_classes > 1:\n",
        "     # Use the index for 'pneumonia' label as the positive class index\n",
        "     # MedMNIST labels are usually 0 for Normal, 1 for Pneumonia\n",
        "     positive_class_label_name = 'pneumonia'\n",
        "     # Find the index corresponding to 'pneumonia'\n",
        "     positive_class_idx = -1\n",
        "     for idx, name in info['label'].items():\n",
        "         if name.lower() == positive_class_label_name:\n",
        "             # Convert the string index to an integer\n",
        "             positive_class_idx = int(idx)\n",
        "             break\n",
        "\n",
        "     if positive_class_idx != -1:\n",
        "         y_prob_epoch.extend(probs_val[:, positive_class_idx].cpu().numpy())\n",
        "     else:\n",
        "         # Fallback or error if 'pneumonia' label is not found\n",
        "         print(f\"Warning: Label '{positive_class_label_name}' not found in dataset info.\")\n",
        "         # You might need to inspect the dataset labels to determine the positive class index\n",
        "         # For PneumoniaMNIST, it's typically 1.\n",
        "         y_prob_epoch.extend(probs_val[:, 1].cpu().numpy()) # Assuming 1 is the positive class index\n",
        "else:\n",
        "     # Handle case with only one class if necessary, though unlikely for this problem\n",
        "     y_prob_epoch.extend(probs_val[:, 0].cpu().numpy()) # Or handle differently\n",
        "\n",
        "\n",
        "epoch_val_acc = running_correct_val / running_total_val\n",
        "print(f\"Validation Accuracy: {epoch_val_acc:.4f}\")\n",
        "\n",
        "# Calculate validation F1 and AUC if there are at least two classes\n",
        "if num_classes > 1 and len(set(y_true_epoch)) > 1:\n",
        "    try:\n",
        "        # Use the index for 'pneumonia' label as the positive class index for binary metrics\n",
        "        positive_class_label_name = 'pneumonia'\n",
        "        positive_class_idx = -1\n",
        "        for idx, name in info['label'].items():\n",
        "            if name.lower() == positive_class_label_name:\n",
        "                # Convert the string index to an integer\n",
        "                positive_class_idx = int(idx)\n",
        "                break\n",
        "\n",
        "        if positive_class_idx != -1:\n",
        "             val_f1 = f1_score(y_true_epoch, y_pred_epoch, average='binary', pos_label=positive_class_idx)\n",
        "             # roc_auc_score requires scores/probabilities for the positive class\n",
        "             val_auc = roc_auc_score(y_true_epoch, y_prob_epoch)\n",
        "             print(f\"Validation F1 Score: {val_f1:.4f}, Validation AUC: {val_auc:.4f}\")\n",
        "        else:\n",
        "             print(f\"Could not calculate validation metrics: Label '{positive_class_label_name}' not found in dataset info.\")\n",
        "\n",
        "\n",
        "    except ValueError as e:\n",
        "         print(f\"Could not calculate validation metrics: {e}. This can happen if only one class is present in the validation batch for binary metrics.\")\n",
        "\n",
        "\n",
        "# Evaluation on Validation Set (Final)\n",
        "print(\"\\nFinal Validation Results:\")\n",
        "model.eval()\n",
        "y_true_val_final, y_pred_val_final, y_prob_val_final = [], [], []\n",
        "with torch.no_grad():\n",
        "    # Indent the following lines\n",
        "    for inputs, labels in val_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        # Squeeze validation labels as well for final evaluation\n",
        "        if labels.ndim > 1 and labels.size(-1) == 1:\n",
        "            labels = labels.squeeze(-1)\n",
        "\n",
        "        outputs = model(inputs)\n",
        "        probs = torch.softmax(outputs, dim=1)\n",
        "        preds = torch.argmax(probs, dim=1)\n",
        "        y_true_val_final.extend(labels.cpu().numpy())\n",
        "        y_pred_val_final.extend(preds.cpu().numpy())\n",
        "        if num_classes > 1:\n",
        "            positive_class_label_name = 'pneumonia'\n",
        "            positive_class_idx = -1\n",
        "            for idx, name in info['label'].items():\n",
        "                if name.lower() == positive_class_label_name:\n",
        "                    # Convert the string index to an integer\n",
        "                    positive_class_idx = int(idx)\n",
        "                    break\n",
        "            if positive_class_idx != -1:\n",
        "                 y_prob_val_final.extend(probs[:, positive_class_idx].cpu().numpy())\n",
        "\n",
        "\n",
        "# Calculate final validation metrics\n",
        "if num_classes > 1 and len(set(y_true_val_final)) > 1:\n",
        "    try:\n",
        "        positive_class_label_name = 'pneumonia'\n",
        "        positive_class_idx = -1\n",
        "        for idx, name in info['label'].items():\n",
        "             if name.lower() == positive_class_label_name:\n",
        "                 # Convert the string index to an integer\n",
        "                 positive_class_idx = int(idx)\n",
        "                 break\n",
        "\n",
        "        if positive_class_idx != -1:\n",
        "             print(\"F1 Score:\", f1_score(y_true_val_final, y_pred_val_final, average='binary', pos_label=positive_class_idx))\n",
        "             print(\"AUC:\", roc_auc_score(y_true_val_final, y_prob_val_final))\n",
        "        else:\n",
        "             print(f\"Could not calculate final validation metrics: Label '{positive_class_label_name}' not found in dataset info.\")\n",
        "    except ValueError as e:\n",
        "         print(f\"Could not calculate final validation metrics: {e}\")\n",
        "else:\n",
        "    print(\"Cannot calculate F1 and AUC for validation set (less than 2 classes or only one class in true labels).\")\n",
        "\n",
        "\n",
        "# Evaluation on Test Set\n",
        "print(\"\\nTest Results:\")\n",
        "model.eval()\n",
        "y_true_test, y_pred_test, y_prob_test = [], [], []\n",
        "with torch.no_grad():\n",
        "    # Indent the following lines\n",
        "    for inputs, labels in test_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        # Squeeze test labels as well\n",
        "        if labels.ndim > 1 and labels.size(-1) == 1:\n",
        "            labels = labels.squeeze(-1)\n",
        "\n",
        "        outputs = model(inputs)\n",
        "        probs = torch.softmax(outputs, dim=1)\n",
        "        preds = torch.argmax(probs, dim=1)\n",
        "        y_true_test.extend(labels.cpu().numpy())\n",
        "        y_pred_test.extend(preds.cpu().numpy())\n",
        "        if num_classes > 1:\n",
        "            positive_class_label_name = 'pneumonia'\n",
        "            positive_class_idx = -1\n",
        "            for idx, name in info['label'].items():\n",
        "                if name.lower() == positive_class_label_name:\n",
        "                    # Convert the string index to an integer\n",
        "                    positive_class_idx = int(idx)\n",
        "                    break\n",
        "\n",
        "            if positive_class_idx != -1:\n",
        "                y_prob_test.extend(probs[:, positive_class_idx].cpu().numpy())\n",
        "\n",
        "\n",
        "# Calculate test metrics\n",
        "if num_classes > 1 and len(set(y_true_test)) > 1:\n",
        "    try:\n",
        "        positive_class_label_name = 'pneumonia'\n",
        "        positive_class_idx = -1\n",
        "        for idx, name in info['label'].items():\n",
        "            if name.lower() == positive_class_label_name:\n",
        "                # Convert the string index to an integer\n",
        "                positive_class_idx = int(idx)\n",
        "                break\n",
        "\n",
        "        if positive_class_idx != -1:\n",
        "             print(\"F1 Score:\", f1_score(y_true_test, y_pred_test, average='binary', pos_label=positive_class_idx))\n",
        "             print(\"AUC:\", roc_auc_score(y_true_test, y_prob_test))\n",
        "        else:\n",
        "             print(f\"Could not calculate test metrics: Label '{positive_class_label_name}' not found in dataset info.\")\n",
        "\n",
        "    except ValueError as e:\n",
        "         print(f\"Could not calculate test metrics: {e}\")\n",
        "else:\n",
        "     print(\"Cannot calculate F1 and AUC for test set (less than 2 classes or only one class in true labels).\")"
      ],
      "metadata": {
        "id": "k2H5KpY-IbXh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2373fc0c-f50d-4289-cbfb-f0410c99f764"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.7812\n",
            "Validation F1 Score: 0.8679, Validation AUC: 0.9427\n",
            "\n",
            "Final Validation Results:\n",
            "F1 Score: 0.8758465011286681\n",
            "AUC: 0.9116823764638675\n",
            "\n",
            "Test Results:\n",
            "F1 Score: 0.7842741935483871\n",
            "AUC: 0.8767806267806268\n"
          ]
        }
      ]
    }
  ]
}